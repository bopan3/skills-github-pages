---
title: "扩散模型原理"
date: 2024-05-01
---
本文从一个初学者的角度介绍扩散模型原理。
# 1.生成模型
首先扩散模型是一类生成模型，近年来因为其一些优秀的性质在各种类型的生成任务上取得了很好的效果。那什么是生成模型呢？字面上来说，生成模型就是一个能够生成样本的模型。比如说针对 $R$ 这个一维空间，我们有一个生成模型 $M$，每次对这个模型采样，我们就能够生成一个一维向量样本 $\boldsymbol{x}$。而生成模型 $M$ 则决定了 $R$ 这个空间中每个点被采样为样本的可能性，即概率分布 $p(\boldsymbol{x})$. 

我们可以看一个简单的生成模型例子，正态分布 $X \sim N(0, 1)$：

$$
p(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right) \tag{1.1}
$$

这里我们直接写出了该生成模型概率分布的$p(x)$的解析形式。那么我们如何基于这个解析形式去做样本的采样生成呢？一种最基本的思路是[逆变换采样法](https://zh.wikipedia.org/zh-cn/%E9%80%86%E5%8F%98%E6%8D%A2%E9%87%87%E6%A0%B7),即通过求出概率分布的 $p(x)$ 的累积分布函数 $F(x)$ 再求逆函数得到 $F^{-1}(x)$。然后我们只需要从最简单的$Z \sim U(0, 1)$ 中采样一个$z$, 再进行逆变换即可得到目标样本 $ x = F^{-1}(z)$. 

一般拿到/学到了一个生成模型后，我们主要可以应用它做两个事。1.采样新的样本（即做生成）。2.估计一个样本的可能性（Likelihood）。当然，面对现实中各种复杂的分布（如自然图片），直接对$p(x)$建模是非常困难的，即使我们把这个模型建出来了，直接对这么复杂的模型做逆变换采样也很难做到，因此，我们需要构造一些巧妙的结构来实现生成模型的建模和采样，这个需求推动学术界提出了一系列生成模型，GAN(生成对抗网络)、VAE（变分自编码器）、扩散模型就是其中的几种生成模型建模方式。

# 2.VAE生成模型
VAE（变分自编码器）给出了一种基于Maximum Likelihood（最大化模型对观察到的样本的likelihood估计）进行生成模型建模的方法，并给出了相应的采样方法。既然我们难以直接对$p(x)$进行建模和采样，那么一个相应的想法是：我们能否在一个足够简单的分布 $Z$ 中进行采样，然后让神经网络自己去学如何把 $Z$ 中采样到的每一个样本 $z$ 映射成目标分布中的样本 $x$，从而实现在复杂真实分布 $X$ 中的采样？听起来这个想法挺有道理的，把最难的一步丢给了神经网络，把简单分布的采样留给了自己，而且强迫神经网络在简单的分布和复杂的分布直接建立联系，这就是强迫智能的产生呀（压缩即智能），强迫神经网络从复杂的数据分布中找出本质的简单的规律！但是要怎么基于这个想法去构造一个神经网络的的训练框架呢？这就是VAE要解决的问题。

首先,驱动模型参数$\theta$更新的动力是最大化观察到的样本集 $x^{(i)}$ 的可能性。即最优参数$\theta^{*}$为:
$$ \theta^{*}=\arg \max _{\theta} \prod_{i=1}^{n} p_{\theta}\left(x^{(i)}\right)  \tag{2.1} $$

为了把连乘变成连加（这样子每个单样本在总体优化中占的权重就都一样了），我们把可能性log一下：
$$ \theta^{*}=\arg \max _{\theta} \sum_{i=1}^{n} \log p_{\theta}\left(x^{(i)}\right) \tag{2.1} $$

这里我们把$\log p_{\theta}(x^{(i)})$ 叫做证据（evidence），即观察到的数据的log Likelihood。理论上，我们可以通过如下方式计算$\ p_{\theta}(x^{(i)})$:
$$p_{\theta}\left(\mathbf{x}^{(i)}\right)=\int p_{\theta}\left(\mathbf{x}^{(i)} \mid \mathbf{z}\right) p(\mathbf{z}) d \mathbf{z} = \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} p_{\theta}(\mathbf{x^{(i)}} \mid \mathbf{z}) \tag{2.3} $$

即先基于$z$的先验分布$p(z)$采样$z$，再基于条件$z$计算likelihood $p_{\theta}({x}^{(i)} \mid {z})$。但是这样计算实在是太贵了，为了估计一个样本的likelihood需要把所有的$z$都过一遍。（注：不过理论上基于这个公式应该已经能训练一个生成模型了，我们可以把$p(z)$设为标准正态分布，把$\ p_{\theta}(x^{(i)} \mid z)$视为一个确定性映射，用参数为$\theta$的神经网络表示，用随机梯度下降优化$\theta$使得$ - \sum_{i=1}^{n} \log  \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} p_{\theta}(x^{(i)} \mid z)$尽可能小即可。有机会可以实现一下试试看，估计收敛会很慢，但也会有一定效果。）










---------
比如如果我们把
$$ p(x) = \frac{p(x｜z)p(z)}{p(z｜x)} \tag{2} $$